---
author: zjmantou
title: Map
time: 2023-10-19 周四
tags:
  - Android
  - 技术
  - 源码
---
# Android-28版本

引入了红黑树的数据结构和扩容的优化

UML类图

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648996907480-34b90b19-0d1a-4a65-b31c-ea71bd66430c.jpeg)

**HashMap**是**基于哈希表实现的Map接口**。此实现提供了**所有可选的映射操作**，并且允许**空键**和**空值**，要注意的是，**最多允许一条记录的键是空**，**允许多条记录的值是空**。**它不保证映射的顺序随着时间保持不变**。

两个影响性能的参数：**初始容量（initial capacity）和负荷系数（load factor）**，默认capacity为16，负荷系数为0.75，当条目数量超过当前容量和负荷系数的乘积，则会扩容，扩容一般大约时原来的两倍

threshold是HashMap所能容纳的最大数据量的节点（Node）个数，在设置初始容量时应当考虑映射中的最大条目书及其负荷系数，以便减少扩容的次数。如果初始容量大雨threashold除以负荷系数，则不会发生重新散列操作就是不会发生扩容

如果要在一个**HashMap**实例中存储许多**映射**，那么**以足够大的容量创建它**将比**根据需要让映射执行自动重新散列以增长表**更有效地存储**映射**

**HashMap**是使用**链地址法**来解决**哈希冲突**的**（数组与列表的结合）**

首先调用**key**的**hashCode**方法得到**哈希值**，然后再通过**哈希算法**的**后两步运算（高位运算、取模运算）来定位该键值对**对应的**存储位置**，如果**两个key定位到相同的存储位置**，表示发生了**哈希碰撞**，**哈希算法**的计算结果**越分散均匀**，**哈希碰撞**的几率就**越低**，**Map**的**存取效率**就**越高**。

**tableSizeFor方法，返回给定目标的2的幂**

```
// HashMap.java
// 返回给定目标容量的2的幂
static final int tableSizeFor(int cap) {
    int n = cap - 1; // 第一步：首先把传入的给定目标容量减1，然后赋值给n
    n |= n >>> 1; // 第二步：首先n的补码无符号右移1位，然后与原来的n的补码执行或运算，最后赋值给n
    n |= n >>> 2; // 第三步：首先n的补码无符号右移2位，然后与原来的n的补码执行或运算，最后赋值给n
    n |= n >>> 4; // 第四步：首先n的补码无符号右移4位，然后与原来的n的补码执行或运算，最后赋值给n
    n |= n >>> 8; // 第五步：首先n的补码无符号右移8位，然后与原来的n的补码执行或运算，最后赋值给n
    n |= n >>> 16; // 第六步：首先n的补码无符号右移16位，然后与原来的n的补码执行或运算，最后赋值给n
    return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; // 第七步：判断n是否小于0，如果小于0，就返回1；如果大于等于0，就判断n是否大于等于最大容量1073741824，如果大于等于最大容量，就返回最大容量；如果小于最大容量，就返回n加1
}
```

## 添加单个元素

1. 判断table是否已经初始化，如果没有初始化就调用resize方法初始化；
2. 根据key计算出来的哈希值进行计算，得到要插入元素的索引，并且判断该元素的值是否为空，如果是空就创建一个节点，并把数据传进去，然后执行步骤6；
3. 判断该索引所在的元素的数据结构是否是树，如果是，就调用putTreeVal方法，使用红黑树插入数据，然后执行步骤5；
4. 如果该索引所在的元素的数据结果是链表，就执行循环，判断链表是否存在要插入的元素，如果不存在，就创建一个节点，并且插入到链表里面，然后判断是否要将链表转化为红黑树，条件是链表长度是否大于等于8；如果存在，就跳出循环最后执行步骤5；
5. 判断该元素的值是否为空，如果是空就把值赋给它，并返回旧值；
6. 判断数组大小是否大于HashMap所能容纳的最大数据量大节点个数，如果是就扩容，然后返回空。

## 扩容逻辑

1. 判断table数组是否已经初始化，如果初始化就进行扩容，容量是原来的两倍；如果没有初始化，就进行初始化（更新threshol的值和更新table数组）。
2. 对数组进行遍历，按顺序判断步骤3、4、5；
3. 判断该索引的元素是否只有一个，如果是，就把元素放到重新计算的索引所在的位置。
4. 判断该索引的元素的数据结构是否为树，如果是就进行拆分操作。
5. 如果该索引的元素的数据结构为链表，就重新计算索引，重新分组。

1. 如果hash值小于等于旧容量表的，就放在原来位置，如果大于则放到新的位置
2. 新的位置等于原来的位置+旧容量表
3. 把重新排列后的尾巴，next置空。

## 删除

1. 根据key计算出来的哈希值进行取模运算，得到要删除的索引；
2. 如果该索引的key与要删除的元素的key相同，就赋值给node；否则执行步骤3；
3. 如果该索引的元素有下一个节点：

1. 如果数据结构是树，就调用getTreeNode方法
2. 如果是链表，就执行一下逻辑

1. 如果链表中能找到该节点，就赋值给node，并且跳出

4. 如果能找到该节点，就执行以下逻辑：

1. 如果数据结构是树，就调用removeTreeNode方法
2. 如果是链表第一个节点，就把数组的索引指向下一个位置
3. 如果不是链表第一个节点，就删除这个节点（p.next = node.next）

# 题外话

**Map**是一个**接口**，它是**将键映射到值的对象**，**映射不能包含重复的键**，**每个键最多可以映射一个值**，常见的**Map实现类**有**HashMap**、**ConcurrentHashMap**、**Hashtable**、**LinkedHashMap**和**TreeMap**等，它们的**UML类图**如下所示：

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648997214385-a373bf19-00bc-4c1f-a8b4-473212fb99ce.jpeg)

## Collections.synchronizedMap

在内部定义了一个被包装的map的引用，一个对象锁mutex

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648997355146-13172548-dd05-47d2-b1ca-680e295b0912.jpeg)

## ConcurrentHashMap

### 1.7

**ConcurrentHashMap**是**线程安全**的**HashMap**，在**JDK 1.8**之前，**ConcurrentHashMap**引入了**分段锁**，**分段锁**的原理是**将数据分成一段一段存储**，**然后给每一段数据配一把锁**，**当一个线程访问其中一段数据的时候就会占用那把锁**，**但是不影响其他线程访问其他段的数据**，**从而提高效率**；注意点是：1.7的时候get没有加锁，因为HashEntry和里面定义的value和next都用了volatile，可以保证其在多线程之间的可见性，因此可以被多个线程同时读；

>默认情况下，一个 ConcurrentHashMap 会被分成 16 个段，每个段都是一个独立的锁。
>
>这种分段锁的设计使得在多线程环境下，不同的线程可以同时修改不同的数据段，从而减少了锁的竞争。当线程需要修改某个数据段时，只需要获取对应数据段的锁即可，而不需要获取整个 ConcurrentHashMap 的锁。
>
>每个段的长度（Segment size）可以通过构造函数中的 float concurrencyFactor 参数来设置。当 concurrencyFactor 的值越大时，每个段的长度就越小，但同时也会增加更多的段。因此，如果预计的并发访问量较大，可以适当增加 concurrencyFactor 的值，以增加并发性能。
>
>需要注意的是，虽然分段锁减少了锁的竞争，但在某些情况下仍然可能产生死锁。例如，当两个线程需要修改不同的数据段时，如果它们按照相同的顺序获取锁，就可能导致死锁。因此，在使用 ConcurrentHashMap 时，也需要避免产生死锁的情况。

#### size：

先采用不加锁的方式，连续计算元素的个数，最多计算3次：

1、如果前后两次计算结果相同，则说明计算出来的元素个数是准确的；

2、如果前后两次计算结果都不同，则给每个 Segment 进行加锁，再计算一次元素的个数；

### 1.8

在**JDK 1.8**之后，抛弃了**分段锁**，加入了红黑树，利用**内置锁synchronized**和**CAS（Compare And Swap）来保证线程安全**。

分段锁：ConcurrentHashMap 内部将数据分成很多段(Segment)，默认情况下，一个ConcurrentHashMap 会被分成16个段。每个段都是一个独立的锁，这样就可以实现并发修改不同的数据段，减少锁竞争。当线程需要修改某个数据段时，只需要获取对应数据段的锁即可，而不需要获取整个ConcurrentHashMap 的锁。
CAS操作：CAS（Compare and Swap）是一种无锁算法，可以避免长时间等待锁的释放。ConcurrentHashMap 使用CAS操作来保证数据的安全性。CAS操作包含三个操作数：一个内存位置V、预期原值A和新值B。如果内存位置V的值与预期原值A相匹配，则将内存位置的值更新为B，否则不进行任何操作。在并发环境中，如果多个线程尝试更新同一个位置的值，CAS操作可以确保只有一个线程可以成功更新该位置的值。
ConcurrentHashMap 1.8 的数据结构主要由数组+链表/红黑二树构成。在低并发情况下，数据会以链表的形式存储；而在高并发情况下，数据会以红黑树的形式存储。这是因为链表在查找时具有较好的性能，而红黑树在插入和删除时具有较好的性能。

#### Size：

volatile类型的变量baseCount记录元素的个数，当插入新数据或则删除数据时，会通过addCount()方法更新baseCount

### 优缺点

好处是在保证合理的同步前提下，效率很高。坏处是严格来说读取操作不能保证反映最近的更新。例如线程A调用putAll写入大量数据，期间线程B调用get，则只能get到目前为止已经顺利插入的部分数据。

### 区别

1、jdk1.7是ReentrantLock+Segment+HashEntry，而jdk1.8中是使用了synchronized+CAS+HashEntry+红黑树。

2、jdk1.8锁的粒度是HashEntry、jdk1.7锁的粒度是segment、包含多个HashEntry、因此jdk1.8中降低了锁的粒度

3、jdk1.8中使用synchronized进行同步的、因此不需要分段锁

4、jdk1.8使用了红黑树来优化链表、当阈值大于8链表转换为红黑树、红黑树遍历效率高于链表

### 面试题

#### **1. ConcurrentHashMap中变量使用final和volatile修饰有什么用呢？**  
Final域使得确保初始化安全性（initialization safety）成为可能，初始化安全性让不可变形对象不需要同步就能自由地被访问和共享。  
使用volatile来保证某个变量内存的改变对其他线程即时可见，在配合CAS可以实现不加锁对并发操作的支持。get操作可以无锁是由于Node的元素val和指针next是用volatile修饰的，在多线程环境下线程A修改结点的val或者新增节点的时候是对线程B可见的。  
  
#### **2.我们可以使用CocurrentHashMap来代替Hashtable吗？**  
我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。它们都可以用于多线程的环境，但是当Hashtable的大小增加到一定的时候，性能会急剧下降，因为迭代时需要被锁定很长的时间。因为ConcurrentHashMap引入了分割(segmentation)，不论它变得多么大，仅仅需要锁定map的某个部分，而其它的线程不需要等到迭代完成才能访问map。简而言之，在迭代的过程中，ConcurrentHashMap仅仅锁定map的某个部分，而Hashtable则会锁定整个map。

#### **3. ConcurrentHashMap有什么缺陷吗？**  
ConcurrentHashMap 是设计为非阻塞的。在更新时会局部锁住某部分数据，但不会把整个表都锁住。同步读取操作则是完全非阻塞的。好处是在保证合理的同步前提下，效率很高。**坏处是严格来说读取操作不能保证反映最近的更新**。例如线程A调用putAll写入大量数据，期间线程B调用get，则只能get到目前为止已经顺利插入的部分数据。  
  
#### **4. ConcurrentHashMap在JDK 7和8之间的区别**

- JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）
- JDK1.8版本的数据结构变得更加简单，使得操作也更加清晰流畅，因为已经使用synchronized来进行同步，所以不需要分段锁的概念，也就不需要Segment这种数据结构了，由于粒度的降低，实现的复杂度也增加了
- JDK1.8使用红黑树来优化链表，基于长度很长的链表的遍历是一个很漫长的过程，而红黑树的遍历效率是很快的，代替一定阈值的链表，这样形成一个最佳拍档

#### 5、为什么容量都是2的次幂？
- 保证计算hash时的值能够充分离散，减少hash碰撞，因为计算方式是(n-1)&hash,n是容量
- 扩容是2倍是能够使得已经离散的数据计算保持一致。

#### 6、JDK1.8为什么使用内置锁synchronized来代替重入锁ReentrantLock？？

1、因为粒度降低了，在相对而言的低粒度加锁方式，synchronized并不比ReentrantLock差，在粗粒度加锁中ReentrantLock可能通过Condition来控制各个低粒度的边界，更加的灵活，而在低粒度中，Condition的优势就没有了

2、synchronized之前一直都是重量级的锁，但是后来java官方是对他进行过升级的，他现在采用的是锁升级的方式去做的。

针对 synchronized 获取锁的方式，JVM 使用了锁升级的优化方式，就是先使用偏向锁优先同一线程然后再次获取锁，如果失败，就升级为 CAS 轻量级锁，如果失败就会短暂自旋，防止线程被系统挂起。最后如果以上都失败就升级为重量级锁。

所以是一步步升级上去的，最初也是通过很多轻量级的方式锁定的。具体参考Synchronize原理篇[[Synchronize原理]]

---


版权声明：本文为CSDN博主「零食爱好者a」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：https://blog.csdn.net/m0_51332138/article/details/118176318

## Hashtable

**Hashtable**是**遗留类**，它和**HashMap**很相似，**不同的是**，它是继承**Dictionary类**，而且它是**线程安全**的，但是**并发性**不如**ConcurrentHashMap**，**不建议使用该类**，如果需要**线程安全**，可以选择使用**ConcurrentHashMap**。

## LinkedHashMap

**LinkedHashMap**是**HashMap**的**子类**，它通过维护一个**双向链表**来保证**迭代顺序**，这个**迭代顺序**会根据**accessOrder（布尔值）来判断是插入顺序**，还是**访问顺序**，**默认实现是按插入顺序排序的**，它可以实现**LRU（Least Recently Used）算法**。

## TreeMap

**TreeMap**基于**红黑树**的**NavigableMap**实现，它可以根据**键**的**可比较的自然顺序**进行**排序**，或者通过它在**创建**的时候提供的**比较器（Comparator）进行排序**，具体取决于使用的**构造函数**。

# 解决哈希冲突的几种方式

## 开放地址法

**开放地址法**是指当发生**地址冲突**时，按照某种方法继续探测**哈希表**中的其他**存储单元**，直到找到**空位置**为止。**公式**是**Hi(key) = (H(key) + di) mod m**，其中，**H(key)是key的哈希地址**，**di是每次再探测时的地址增量**，**m是哈希表的长度**。

**增量di**可以用**不同的取法**，根据**取法**的不同有如下**名称**：

## 线性探测法

**线性探测法**的**增量di**取**1, 2, 3, ……, k(k <= m - 1)的值，当发生地址冲突**时，在**哈希表**中**顺序**探测下一个**存储单元**，直到找到**空位置**为止。

## 二次探测法

**二次探测法**的**增量di**取**1^2, -1^2, 2^2, -2^2,……, k^2, -k^2(k <= m / 2)**，当发生**地址冲突**时，在**哈希表**的**左右**进行**跳跃式探测**，**双向**探测**空位置**。

## 随机探测法

**随机探测法**的**增量di**是用**随机函数**计算得到，当发生**地址冲突**时，在**哈希表**中**随机**探测**空位置**。

值得一提的是，**ThreadLocal**的**内部类ThreadLocalMap**是采用**开放地址法**来解决**哈希冲突**。

## 链地址法（HashMap）

**链地址法**是指将所有**哈希地址相同**的记录都链接**同一个链表**中，它处理**冲突**简单，而且**没有堆积现象**，也就是**非同义词绝对不会发生冲突**，各**链表**上的**节点**的**空间**都是**动态申请**的，所以更加适合**无法确定哈希表长度的情况**。

## 再哈希法

**再哈希法**是指同时构造多个**不同**的**哈希函数**，当使用其中一个**哈希函数**发生**冲突**时，就使用**另外一个哈希函数**，直到不再发生**冲突**为止，这种方法**不易产生聚集**，但是会**增加计算时间**。

## 建立公共溢出区

**建立公共溢出区**是指将**哈希表**分为**基本表**和**溢出表**，凡是和**基本表**发生**冲突**的**元素**都会被填入**溢出表**，而且**溢出表**也可以使用同样的**哈希函数**，**易于实现**。

# HashMap 和 Hashtable 区别

这个一定要去看源码！看源码！看源码！实在看不下去的可以上网看别人的分析。简单总结有几点：

1. HashMap 支持 null Key 和 null Value；Hashtable 不允许。这是因为 HashMap 对 null 进行了特殊处理，将 null 的 hashCode 值定为了 0，从而将其存放在哈希表的第 0 个 bucket。
2. HashMap 是非线程安全，HashMap 实现线程安全方法为 Map map = Collections.synchronziedMap(new HashMap())；HashTable 是线程安全
3. HashMap 默认长度是 16，扩容是原先的 2 倍；Hashtable 默认长度是 11，扩容是原先的 2n+1
4. HashMap 继承 AbstractMap；HashTable 继承了 Dictionary

扩展，HashMap 对比 ConcurrentHashMap ，HashMap 对比 SparseArray，LinkedArray 对比 ArrayList，ArrayList 对比 Vector

# SparseArray

SparseArray构造方法中，创建了两个数组mKeys、mValues分别存放int与Object，其默认长度为10

## put

- 因为key为int,不存在hash冲突
- mKeys为有序列表，通过二分查找，找到要插入的key对应mKeys数组中的index (这里相对于查找hash表应该算是费时间吧，但节省了内存，所以是 时间换取了空间)
- 通过key对应mKeys数组中的index，将Value插入到mValues数组的index对应位置  
    插入数据过程中：  
    1、如果mValues数组index位置的数据已经删除，则直接插入；  
    2、如果mValues数组index位置存在有效数据，或者数组长度不足了，则需要查看`GrowingArrayUtils.insert`代码了. 
`GrowingArrayUtils.insert`:
- 如果mValues数组index位置存在有效数据，而且数组长度够。则将mValues数组index位置后的元素都向后移动一位，index位置存入对应的element
- 如果长度不够，则扩容到原长度的两倍，并将其他数据复制到新数组中

## get

每次调用get，则需经过一次mKeys数组的二分查找，因此mKeys数组越大则二分查找的时间就越长，因此SparseArray在大量数据，千以上时，会效率较低



# 红黑树

## 什么是红黑树

红黑树是一种自平衡**二叉**查找树,虽然它的实现非常复杂,但即使是在最坏情况下运行也能有很好的效率.比如当红黑树上有N个元素时,它可以在O(log N)时间内做查找,插入和删除.

作为一种查找树,它需要符合一些规则:

1. 若左子树不空，则左子树上所有结点的值均小于它的根结点的值；
2. 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值；
3. 左、右子树也分别为二叉排序树；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998282195-48364897-4386-4534-a607-e8858a2b6968.jpeg)

## 红黑树的5个性质

1. 在红黑树中,每个节点都有颜色,要么是红的,要么是黑的；
2. 红黑树的根节点是黑色的；
3. 每个叶子节点(在 Java 中 为null)都是黑色的；
4. 如果一个节点是红色的,那么它的子节点都是黑色的；
5. 从任意节点到每个叶子节点的路径上,黑色节点的数目相同；

因为插入,查找,删除操作时,**最坏的情况**下的时间都与二叉树的**树高**有关,根据性质 4 我们知道不会有两个直接相连的红色节点.接着,根据性质 5 我们又可以知道,因为所有最长的路径都有相同数目的黑色节点,这就保证了没有可能会有一条路径的长度能有其他路径的两倍这么长。

## 树化

treeifyBin()

1. 先判断容量是否大于等于变成树的最小容量（MIN_TREEIFY_CAPACITY = 64）如果没有的话就扩容解决；
2. 将当前链表转换成双向链表（生成TreeNode，将原来链表从表头开始依次给treeNode.prev和treeNode.next赋值）
3. 以该双向链表的表头为该Node数组的头（即红黑树的根节点）开始树化；

1. 从头开始遍历，如果小于跟节点放在左边，大于等于跟节点放在右边；

4. 最后把红黑树的根节点移动成为桶的第一个元素；

## 左旋与右旋

左旋：

1. 选定一个节点 N 作为左旋操作的支点；
2. 该节点 N 代替 N 原本的父节点P的位置；
3. 原本的父节点 P 变成 N 的左子节点；
4. 如果 N 原本有左子节点,那么这个左子节点现在变成P的右子节点；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998562511-cdce0c4a-c8d6-4881-b7c3-245ae284904c.jpeg)

右旋：

1. 选定一个节点 N 作为右旋操作的支点；
2. 该节点 N 代替 N 原本的父节点P的位置；
3. 原本的父节点 P 变成 N 的右子节点；
4. 如果 N 原本有右子节点,那么这个右子节点现在变成 P 的左子节点；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998591105-abbc0b3b-019b-4730-9a2f-38dc5605ddc2.jpeg)

## 增加元素

两个步骤：

- 把红黑树当做二叉查找树做**插入**操作
- 把插入后的二叉查找树重新**调整**成红黑树

核心思想：将影响红黑树性质的红色节点向上移动到根节点,并将根节点设置成黑色

具体参考链接：[https://juejin.cn/post/6844903825011113991](https://juejin.cn/post/6844903825011113991)

putTreeVal源码分析：

1. 从跟节点开始遍历，如果遍历到的节点的Hash值大于要插入的Key的Hash值，则向左子树继续查找，反之向右子树；
2. 如果Hash值相等且Key也相等，返回这个节点并在PutVal里面进行修改；否则就在节点的子树里面查找是否有匹配的节点，有就返回这个节点，没有就使用对象地址进行比较（返回一个dir）；
3. 根据hash值比较结果（dir）：如果dir<=0（dir>0），若果左子树（右子树）为null，就插入新节点，进行平衡处理（balanceInsertion(root, x)），不为空就继续向左（向右子树进行查询）。

### **平衡树处理**

新节点初始化为红色，x->当前节点，xp->父节点，xpp->祖父节点，xppl、xppr->分别是祖父节点的左右子节点：

1. 如果是空树，则x涂黑并返回；
2. 如果xp是黑色，且xpp=null，返回根节点；
3. 如果xp==xppl，父节点是祖父节点的左子节点；

1. 如果xppr红色，三红情况，变色，xp和xppr黑色，xpp红色；
2. 否则，看自己是否是左子节点还是右子节点；

1. 左子节点：xp变黑，xpp变红，右旋；
2. 右子节点：将x更新为xp，左旋，再更新xp和xpp；

4. 如果xp==xppr，父节点是父节点的右子节点，与步骤3一样，只不过左右互换。

## 删除元素

1. 无子节点时，删除节点可能为红色或者黑色；

1. 如果为红色，直接删除即可，不会影响黑色节点的数量；
2. 如果为黑色，则需要进行删除平衡的操作了；

2. 只有一个子节点时，删除节点只能是黑色，其子节点为红色，否则无法满足红黑树的性质了。 此时用删除节点的子节点接到父节点，且将子节点颜色涂黑，保证黑色数量。
3. 有两个子节点时，与二叉搜索树一样，使用后继节点作为替换的删除节点，情形转至为1或2处理。

总结：两种情况，删除红色节点，直接删除，如果删除黑色节点，需要进行删除平衡操作；

删除平衡：balanceDeletion

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998729633-15765bbb-e76e-4399-9331-f13cc15370e2.jpeg)

平衡的两种方式：当前节点黑色+1，兄弟节点黑色-1；

1. 当前节点为根节点，无需平衡；
2. 兄弟节点为黑色（S=黑色）

1. 兄弟子节点全黑（SL = SR = 黑），意味着兄弟节点（S）可以涂红而不会和子冲突，就平衡了，具体看父节点（P）

1. 父节点为黑色（P=黑），S涂红，完成平衡；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998780290-6f993e52-1075-43f5-9b4a-19e4c83763ff.jpeg)

2. 父节点为红色，P涂黑，S涂红，完成平衡；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998828450-ee0b0d9d-edb4-4b8e-bb91-6d35381c8017.jpeg)

2. 兄弟子节点不全黑（不全黑包括：[SL红, SR红]、[SL黑，SR红]、[SL红，SR黑]。如果其中一个为黑，另外一个肯定是红）

1. S为左子，SL=红：以P为支点右旋，SL涂黑，平衡结束；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998887401-dde6651a-93fa-49fb-ac1f-9da93834bd1f.jpeg)

2. S为右子，SR=红：以P为支点左旋，交换P和S的颜色，SR涂黑；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998922101-165c7bf6-26d6-40bd-a8ed-4bc9d895a333.jpeg)

3. S为左子，SL=黑：以S为支点左旋，交换S和SR的颜色（SR涂黑，S涂红），在转至i处理（S涂红是保证了原S至SR的黑色数量一致）；

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648998953733-ec07ee88-331c-4db7-9008-19c2ed5d5e36.jpeg)

4. S为右子，SR=黑：以S为支点右旋，交换S和SL的颜色（SL涂黑，S涂红），此时转至ii处理；

3. 兄弟节点为红色（S=红色），

1. S为左子时，以P进行右旋，旋转后交换P和S的颜色（S涂黑，P涂红），N兄弟节点变为黑色，进入情形2-兄弟节点为黑色进行处理；
2. S为右子时，以P进行左旋，旋转后交换P和S的颜色（S涂黑，P涂红），N兄弟节点变为黑色，进入情形2-兄弟节点为黑色进行处理。

![](https://cdn.nlark.com/yuque/0/2022/jpeg/26044650/1648999031197-6accca89-d93d-4219-9823-daa52643b46d.jpeg)

### 删除动作总结与实例

1. 删除动作之后，看看这个节点是不是黑色的叶子结点，如果不是，简单处理就能平衡；
2. 先看N是不是根节点，是的话不用管；不是的话看兄弟是什么颜色；

1. 兄弟是红色：去到兄弟为黑色那处理
2. 兄弟是黑色：看看兄弟子节点是不是全黑：

1. 全黑的话，看父节点什么颜色进行对应处理；
2. 不全黑，看兄弟在的位置，左边的话看兄弟的左子节点是不是红色，进行对应处理；兄弟是右边的话，看看兄弟的右子节点是不是红色，进行处理；

作者：路过的猪

链接：https://www.jianshu.com/p/84416644c080

来源：简书

著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 查找元素

比较简单，getTreeNode里面调用了TreeNode里面的find方法；

## 为什么长度是8才转成红黑树

```

In usages with well-distributed user hashCodes, tree bins 
are rarely used.  Ideally, under random hashCodes, the 
frequency of nodes in bins follows a Poisson distribution 
(http://en.wikipedia.org/wiki/Poisson_distribution) with a 
parameter of about 0.5 on average for the default resizing 
threshold of 0.75, although with a large variance because 
of resizing granularity. Ignoring variance, the expected 
occurrences of list size k are (exp(-0.5) * pow(0.5, k) / 
factorial(k)). The first values are:
 
 0:    0.60653066
 1:    0.30326533
 2:    0.07581633
 3:    0.01263606
 4:    0.00157952
 5:    0.00015795
 6:    0.00001316
 7:    0.00000094
 8:    0.00000006
 more: less than 1 in ten million
```

上面这段话的意思是，如果 hashCode 分布良好，也就是 hash 计算的结果离散好的话，那么红黑树这种形式是很少会被用到的，因为各个值都均匀分布，很少出现链表很长的情况。在理想情况下，链表长度符合泊松分布，各个长度的命中概率依次递减，当长度为 8 的时候，概率仅为 0.00000006。这是一个小于千万分之一的概率，通常我们的 Map 里面是不会存储这么多的数据的，所以通常情况下，并不会发生从链表向红黑树的转换。

————————————————

版权声明：本文为CSDN博主「Jerrycodes」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：[https://blog.csdn.net/kyle_wu_/article/details/113578055](https://blog.csdn.net/kyle_wu_/article/details/113578055)

# **HashMap 1.7与1.8的区别**

最重要的一点是底层结构不一样，1.7是数组+链表，1.8则是数组+链表+红黑树结构;

2. jdk1.7中当哈希表为空时，会先调用inflateTable()初始化一个数组；而1.8则是直接调用resize()扩容;

3. 插入键值对的put方法的区别，1.8中会将节点插入到链表尾部，而1.7中是采用头插；

4. jdk1.7中的hash函数对哈希值的计算直接使用key的hashCode值，而1.8中则是采用key的hashCode异或上key的hashCode进行无符号右移16位的结果，避免了只靠低位数据来计算哈希时导致的冲突，计算结果由高低位结合决定，使元素分布更均匀；

5. 扩容时1.8会保持原链表的顺序，而1.7会颠倒链表的顺序；而且1.8是在元素插入后检测是否需要扩容，1.7则是在元素插入前；

6. jdk1.8是扩容时通过hash&cap==0将链表分散，无需改变hash值，而1.7是通过更新hashSeed来修改hash值达到分散的目的；

7. 扩容策略：1.7中是只要不小于阈值就直接扩容2倍；而1.8的扩容策略会更优化，当数组容量未达到64时，以2倍进行扩容，超过64之后若桶中元素个数不小于7就将链表转换为红黑树，但如果红黑树中的元素个数小于6就会还原为链表，当红黑树中元素不小于32的时候才会再次扩容。

————————————————

版权声明：本文为CSDN博主「”PANDA」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：https://blog.csdn.net/qq_38685503/article/details/88430788

# ArrayMap

构造方法中初始化了两个数组mHashes、mArray，其中mHashes为key的Hash值对应的数组

线程不安全

**开放地址法**处理hash碰撞

**二分查找法**搜索元素，时间复杂度是0（log N），比HashMap慢，而且数据规模越大，慢的越多；

优点：

节省内存；

缺点：

查找速度慢；

不能跨平台；  

采用缓存机制来解决put和remove时的扩容和减少容量带来的内存回收压力。

ArrayMap 在内存利用率上比 HashMap 更高，因为不用创建额外的 Node 数据结构，同时具有缓存机制，避免频繁创建对象而分配内存与 GC 操作。此外，在数据条目小于容量 1/3 时会触发内存收缩至原理的 0.5 倍。因此，当数据量不大（小于1000）时更推荐使用 ArrayMap。更详细的对比

# 红黑树参考：

作者：_晨曦_

链接：https://juejin.cn/post/6844903825011113991

来源：稀土掘金

著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。